{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr4H6HxUkaZ4U3m+gw2Nmd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryanAhmadChaudhary/NLP/blob/main/Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**\n",
        "\n",
        "Lemmatization in Natural Language Processing (NLP) is the process of reducing a word to its base or root form, called a lemma, while ensuring that it retains its meaning within the context. Unlike stemming, which often uses heuristic rules to chop off word endings, lemmatization takes into account the word's part of speech (POS) and applies linguistic rules to achieve a more accurate base form. <br>\n",
        "POS:\n",
        "- Noun : n\n",
        "- Verb : v\n",
        "- Adjective : a\n",
        "- Adverb : r"
      ],
      "metadata": {
        "id": "JjsV3JaqjbUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "e0ccufnUjfeS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "81POXbXFkzea"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"going\",pos = 'n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QgKZTXqUk9CU",
        "outputId": "12a9fb8a-d2ff-461f-e7c7-1aaf22d62bb6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"going\",pos = 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2qao-vqNlQQl",
        "outputId": "ab5e1f97-442e-4d53-acd9-4a275f506e12"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"eating\", \"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ],
      "metadata": {
        "id": "qHQlxRKBmf9f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word + \" ---> \" + lemmatizer.lemmatize(word, pos = 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J74pRNrmpkp",
        "outputId": "9e56e945-55a7-42a4-fa07-0ec52d7d97bf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eats ---> eat\n",
            "eaten ---> eat\n",
            "writing ---> write\n",
            "writes ---> write\n",
            "programming ---> program\n",
            "programs ---> program\n",
            "history ---> history\n",
            "finally ---> finally\n",
            "finalized ---> finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udEBMJ493Yb4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}